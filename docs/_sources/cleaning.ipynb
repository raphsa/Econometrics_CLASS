{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "916fc54e",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "Data cleaning (also known as data cleansing) is the process of identifying and correcting (or removing) inconsistencies, errors and inaccuracies within a dataset. In a data science or data analysis project, this process is fundamental in ensuring a reliable and effective analysis, since it lands at the beginning of the pipeline. Data cleaning ensures that the data is accurate, consistent, and reliable.\n",
    "\n",
    "In essence, data cleaning involves identifying and resolving flaws within datasets by correcting, removing, or modifying records to meet quality standards suitable for analysis. It plays a critical role in data preprocessing and significantly influences how data will be modeled and interpreted.\n",
    "\n",
    "As an old commercial once said, \"Power is nothing without control\". The same concept applies here. We can be the best data scientist, business analyst, AI engineer, statistician (or whatever role you prefer), but if we don't have high-quality, clean and consistent data, all our models and predictions are worthless and completely useless. \n",
    "\n",
    "### Why Data Cleaning matters\n",
    "\n",
    "- **Improved data quality**: Cleaning data reduces errors, inconsistencies and missing values, leading to more accurate and reliable analysis.\n",
    "- **Enhanced efficiency**: High-quality data is easier and quicker to analyze, reducing the time and effort spent managing poor data quality.\n",
    "- **Better decision-making**: Clean and consistent data provides trustworthy insights, enabling organizations to make well-informed decisions rather than relying on outdated or incomplete data.\n",
    "\n",
    "### Common Data Quality Issues\n",
    "\n",
    "Data quality problems may arise from various sources, such as human errors, system failures, or issues during data integration. Typical data quality issues include:\n",
    "\n",
    "1. **Missing values**: Lack of some data or missing information can result in failure to make the right conclusions and can or else lead to creating a biased result. In order to handle it, different strategies can be adopted:\n",
    "    - Deleting incomplete records (in case they have minimal impact)\n",
    "    - Imputing values (by using estimated ones, like the mean, median or mode)\n",
    "    - Exploiting predictive modeling from the machine learning field to predict the input value\n",
    "2. **Incorrect data types**: Mismatched data types (e.g., strings in numerical fields) can hinder processing and analysis.\n",
    "3. **Anomalies and outliers**: Extreme values can skew results and impact statistical accuracy. Detecting and either correcting, removing, or transforming extreme values preserves the accuracy of insights. Sometimes they may be found due to an error in data collection (e.g. if we have the variable `weight` represented in `kg` and we write down an observation in `g`, leading to an enourmous and anomal value with respect to other observations). Another common situation is related to some typos (e.g. we unintentionally add a 0 at the end of a number, thus recording a 10 time higher number). These anomalies are also known as spelling and typographical errors. Fixing typos or inaccurate values often involves validation checks or cross-referencing with trusted sources. \n",
    "4. **Duplicate data**: Repeated entries can distort results, metrics and produce skewed results. Identifying and eliminating redundant records ensures data integrity and uniqueness.\n",
    "5. **Inconsistent formats**: Disparities in formats (e.g., date formats, text casing) complicate data aggregation and comparison. Ensuring uniformity in formats such as dates, addresses, and phone numbers is essential for an easier analysis.\n",
    "\n",
    "## Challenges in Data Cleaning\n",
    "\n",
    "When trying to clean our source data, different problems can be faced:\n",
    "\n",
    "- Large Volumes: Big datasets require scalable tools and efficient methods, due to their sheer size.\n",
    "- Ongoing Nature: Cleaning is not a one-time task but a continuous process as new data is collected and integrated.\n",
    "- Data Complexity: Data may come from different sources and have diverse formats: these multiple sources can be hard to reconcile."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "source_map": [
   13
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}